{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sentiment_analysis.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"BYVn3F4Eotkz","colab_type":"code","outputId":"9f4a2050-7546-4a11-c305-b43e5ed9b425","executionInfo":{"status":"ok","timestamp":1572560253677,"user_tz":420,"elapsed":19144,"user":{"displayName":"Tanvir Ahmmed","photoUrl":"","userId":"06405482776379779141"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZSdAoz94o6_X","colab_type":"code","colab":{}},"source":["train_data = {\n","  'good': True,\n","  'bad': False,\n","  'happy': True,\n","  'sad': False,\n","  'not good': False,\n","  'not bad': True,\n","  'not happy': False,\n","  'not sad': True,\n","  'very good': True,\n","  'very bad': False,\n","  'very happy': True,\n","  'very sad': False,\n","  'i am happy': True,\n","  'this is good': True,\n","  'i am bad': False,\n","  'this is bad': False,\n","  'i am sad': False,\n","  'this is sad': False,\n","  'i am not happy': False,\n","  'this is not good': False,\n","  'i am not bad': True,\n","  'this is not sad': True,\n","  'i am very happy': True,\n","  'this is very good': True,\n","  'i am very bad': False,\n","  'this is very sad': False,\n","  'this is very happy': True,\n","  'i am good not bad': True,\n","  'this is good not bad': True,\n","  'i am bad not good': False,\n","  'i am good and happy': True,\n","  'this is not good and not happy': False,\n","  'i am not at all good': False,\n","  'i am not at all bad': True,\n","  'i am not at all happy': False,\n","  'this is not at all sad': True,\n","  'this is not at all happy': False,\n","  'i am good right now': True,\n","  'i am bad right now': False,\n","  'this is bad right now': False,\n","  'i am sad right now': False,\n","  'i was good earlier': True,\n","  'i was happy earlier': True,\n","  'i was bad earlier': False,\n","  'i was sad earlier': False,\n","  'i am very bad right now': False,\n","  'this is very good right now': True,\n","  'this is very sad right now': False,\n","  'this was bad earlier': False,\n","  'this was very good earlier': True,\n","  'this was very bad earlier': False,\n","  'this was very happy earlier': True,\n","  'this was very sad earlier': False,\n","  'i was good and not bad earlier': True,\n","  'i was not good and not happy earlier': False,\n","  'i am not at all bad or sad right now': True,\n","  'i am not at all good or happy right now': False,\n","  'this was not happy and not good earlier': False,\n","}\n","\n","test_data = {\n","  'this is happy': True,\n","  'i am good': True,\n","  'this is not happy': False,\n","  'i am not good': False,\n","  'this is not bad': True,\n","  'i am not sad': True,\n","  'i am very good': True,\n","  'this is very bad': False,\n","  'i am very sad': False,\n","  'this is bad not good': False,\n","  'this is good and happy': True,\n","  'i am not good and not happy': False,\n","  'i am not at all sad': True,\n","  'this is not at all good': False,\n","  'this is not at all bad': True,\n","  'this is good right now': True,\n","  'this is sad right now': False,\n","  'this is very bad right now': False,\n","  'this was good earlier': True,\n","  'i was not happy and not good earlier': False,\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4E8tdIPlo9B7","colab_type":"code","colab":{}},"source":["vocab = list(set([w for text in train_data.keys() for w in text.split(' ')]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8An6ZsPkpUH0","colab_type":"code","outputId":"5213f847-2936-40ad-ff92-5e3723e0e666","executionInfo":{"status":"ok","timestamp":1572560261011,"user_tz":420,"elapsed":1232,"user":{"displayName":"Tanvir Ahmmed","photoUrl":"","userId":"06405482776379779141"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["vocab_size = len(vocab)\n","print('%d unique words found' % vocab_size)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["18 unique words found\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QhiFPnfGphsJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":323},"outputId":"1a6642f0-c08b-4251-aee8-d02e31a1c50c","executionInfo":{"status":"ok","timestamp":1572561155973,"user_tz":420,"elapsed":1217,"user":{"displayName":"Tanvir Ahmmed","photoUrl":"","userId":"06405482776379779141"}}},"source":["word_to_idx = { w: i for i, w in enumerate(vocab) }\n","word_to_idx"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'all': 10,\n"," 'am': 11,\n"," 'and': 6,\n"," 'at': 9,\n"," 'bad': 16,\n"," 'earlier': 14,\n"," 'good': 5,\n"," 'happy': 0,\n"," 'i': 17,\n"," 'is': 8,\n"," 'not': 3,\n"," 'now': 2,\n"," 'or': 15,\n"," 'right': 1,\n"," 'sad': 12,\n"," 'this': 7,\n"," 'very': 4,\n"," 'was': 13}"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"ckZZUS9rpmA7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":323},"outputId":"91ba2dfc-767b-45c7-e946-c4c8553fefdf","executionInfo":{"status":"ok","timestamp":1572561180754,"user_tz":420,"elapsed":1233,"user":{"displayName":"Tanvir Ahmmed","photoUrl":"","userId":"06405482776379779141"}}},"source":["idx_to_word = { i: w for i, w in enumerate(vocab) }\n","idx_to_word"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'happy',\n"," 1: 'right',\n"," 2: 'now',\n"," 3: 'not',\n"," 4: 'very',\n"," 5: 'good',\n"," 6: 'and',\n"," 7: 'this',\n"," 8: 'is',\n"," 9: 'at',\n"," 10: 'all',\n"," 11: 'am',\n"," 12: 'sad',\n"," 13: 'was',\n"," 14: 'earlier',\n"," 15: 'or',\n"," 16: 'bad',\n"," 17: 'i'}"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"uVFY8jmVqDmL","colab_type":"code","outputId":"126ea181-7cba-4063-cc7d-e38b7945e3a3","executionInfo":{"status":"ok","timestamp":1572561272710,"user_tz":420,"elapsed":1258,"user":{"displayName":"Tanvir Ahmmed","photoUrl":"","userId":"06405482776379779141"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["import numpy as np\n","\n","def createInputs(text):\n","  '''\n","  Returns an array of one-hot vectors representing the words\n","  in the input text string.\n","  - text is a string\n","  - Each one-hot vector has shape (vocab_size, 1)\n","  '''\n","  inputs = []\n","  for w in text.split(' '):\n","    #print(word_to_idx[w],w)\n","    v = np.zeros((vocab_size, 1))\n","    v[word_to_idx[w]] = 1\n","    print(w,word_to_idx[w],v[word_to_idx[w]])\n","    inputs.append(v)\n","  return inputs\n","inputs=createInputs('i am very good')\n","print(word_to_idx['i'])"],"execution_count":19,"outputs":[{"output_type":"stream","text":["i 17 [1.]\n","am 11 [1.]\n","very 4 [1.]\n","good 5 [1.]\n","17\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cVxvh-Eyrd8S","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5382db83-0b08-4946-87c9-159833de4cf8","executionInfo":{"status":"ok","timestamp":1572561387144,"user_tz":420,"elapsed":1612,"user":{"displayName":"Tanvir Ahmmed","photoUrl":"","userId":"06405482776379779141"}}},"source":["from numpy.random import randn\n","class RNN:\n","  # A Vanilla Recurrent Neural Network.\n","\n","  def __init__(self, input_size, output_size, hidden_size=64):\n","    # Weights\n","    self.Whh = randn(hidden_size, hidden_size) / 1000\n","    self.Wxh = randn(hidden_size, input_size) / 1000\n","    self.Why = randn(output_size, hidden_size) / 1000\n","    #print(self.Whh.shape)\n","    # Biases\n","    self.bh = np.zeros((hidden_size, 1))\n","    self.by = np.zeros((output_size, 1))\n","\n","  def backprop(self, d_y, learn_rate=2e-2):\n","    '''\n","    Perform a backward pass of the RNN.\n","    - d_y (dL/dy) has shape (output_size, 1).\n","    - learn_rate is a float.\n","    '''\n","    n = len(self.last_inputs)\n","\n","    # Calculate dL/dWhy and dL/dby.\n","    d_Why = d_y @ self.last_hs[n].T\n","    d_by = d_y\n","\n","    # Initialize dL/dWhh, dL/dWxh, and dL/dbh to zero.\n","    d_Whh = np.zeros(self.Whh.shape)\n","    d_Wxh = np.zeros(self.Wxh.shape)\n","    d_bh = np.zeros(self.bh.shape)\n","\n","    # Calculate dL/dh for the last h.\n","    d_h = self.Why.T @ d_y\n","\n","    # Backpropagate through time.\n","    for t in reversed(range(n)):\n","      # An intermediate value: dL/dh * (1 - h^2)\n","      temp = ((1 - self.last_hs[t + 1] ** 2) * d_h)\n","\n","      # dL/db = dL/dh * (1 - h^2)\n","      d_bh += temp\n","\n","      # dL/dWhh = dL/dh * (1 - h^2) * h_{t-1}\n","      d_Whh += temp @ self.last_hs[t].T\n","\n","      # dL/dWxh = dL/dh * (1 - h^2) * x\n","      d_Wxh += temp @ self.last_inputs[t].T\n","\n","      # Next dL/dh = dL/dh * (1 - h^2) * Whh\n","      d_h = self.Whh @ temp\n","\n","    # Clip to prevent exploding gradients.\n","    for d in [d_Wxh, d_Whh, d_Why, d_bh, d_by]:\n","      np.clip(d, -1, 1, out=d)\n","\n","    # Update weights and biases using gradient descent.\n","    self.Whh -= learn_rate * d_Whh\n","    self.Wxh -= learn_rate * d_Wxh\n","    self.Why -= learn_rate * d_Why\n","    self.bh -= learn_rate * d_bh\n","    self.by -= learn_rate * d_by\n","\n","  def forward(self, inputs):\n","    '''\n","    Perform a forward pass of the RNN using the given inputs.\n","    Returns the final output and hidden state.\n","    - inputs is an array of one hot vectors with shape (input_size, 1).\n","    '''\n","    h = np.zeros((self.Whh.shape[0], 1))\n","\n","    self.last_inputs = inputs\n","    self.last_hs = { 0: h }\n","    #print(self.last_hs)\n","    # Perform each step of the RNN\n","    for i, x in enumerate(inputs):\n","      h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n","      self.last_hs[i + 1] = h\n","\n","    # Compute the output\n","    y = self.Why @ h + self.by\n","\n","    return y, h\n","rnn = RNN(vocab_size, 2)\n","def softmax(xs):\n","    # Applies the Softmax Function to the input array.\n","    return np.exp(xs) / sum(np.exp(xs))\n"],"execution_count":22,"outputs":[{"output_type":"stream","text":["(64, 64)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Mk4_ivUHSpQy","colab_type":"code","colab":{}},"source":["import random\n","\n","def processData(data, backprop=True):\n","  '''\n","  Returns the RNN's loss and accuracy for the given data.\n","  - data is a dictionary mapping text to True or False.\n","  - backprop determines if the backward phase should be run.\n","  '''\n","  items = list(data.items())\n","  random.shuffle(items)\n","\n","  loss = 0\n","  num_correct = 0\n","\n","  for x, y in items:\n","    inputs = createInputs(x)\n","    target = int(y)\n","\n","    # Forward\n","    out, _ = rnn.forward(inputs)\n","    probs = softmax(out)\n","\n","    # Calculate loss / accuracy\n","    loss -= np.log(probs[target])\n","    num_correct += int(np.argmax(probs) == target)\n","\n","    if backprop:\n","      # Build dL/dy\n","      d_L_d_y = probs\n","      d_L_d_y[target] -= 1\n","\n","      # Backward\n","      rnn.backprop(d_L_d_y)\n","\n","  return loss / len(data), num_correct / len(data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EcC7PeToG4WF","colab_type":"code","outputId":"17592a6c-6350-4c1b-b78b-a37185078899","executionInfo":{"status":"error","timestamp":1572560233819,"user_tz":420,"elapsed":1294,"user":{"displayName":"Tanvir Ahmmed","photoUrl":"","userId":"06405482776379779141"}},"colab":{"base_uri":"https://localhost:8080/","height":163}},"source":["rnn.backprop()"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b758b70d3eed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'rnn' is not defined"]}]},{"cell_type":"code","metadata":{"id":"cvtZkX-ND9B0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":527},"outputId":"225e9c60-5897-48af-d13e-e1fc40ffa32e","executionInfo":{"status":"ok","timestamp":1572561064728,"user_tz":420,"elapsed":24621,"user":{"displayName":"Tanvir Ahmmed","photoUrl":"","userId":"06405482776379779141"}}},"source":["# Training loop\n","for epoch in range(1000):\n","  train_loss, train_acc = processData(train_data)\n","\n","  if epoch % 100 == 99:\n","    print('--- Epoch %d' % (epoch + 1))\n","    print('Train:\\tLoss %.3f | Accuracy: %.3f' % (train_loss, train_acc))\n","\n","    test_loss, test_acc = processData(test_data, backprop=False)\n","    print('Test:\\tLoss %.3f | Accuracy: %.3f' % (test_loss, test_acc))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["--- Epoch 100\n","Train:\tLoss 0.690 | Accuracy: 0.552\n","Test:\tLoss 0.698 | Accuracy: 0.500\n","--- Epoch 200\n","Train:\tLoss 0.672 | Accuracy: 0.569\n","Test:\tLoss 0.724 | Accuracy: 0.500\n","--- Epoch 300\n","Train:\tLoss 0.586 | Accuracy: 0.672\n","Test:\tLoss 0.659 | Accuracy: 0.550\n","--- Epoch 400\n","Train:\tLoss 0.423 | Accuracy: 0.793\n","Test:\tLoss 0.750 | Accuracy: 0.750\n","--- Epoch 500\n","Train:\tLoss 0.347 | Accuracy: 0.845\n","Test:\tLoss 0.878 | Accuracy: 0.600\n","--- Epoch 600\n","Train:\tLoss 0.127 | Accuracy: 0.966\n","Test:\tLoss 0.727 | Accuracy: 0.750\n","--- Epoch 700\n","Train:\tLoss 0.253 | Accuracy: 0.862\n","Test:\tLoss 0.716 | Accuracy: 0.800\n","--- Epoch 800\n","Train:\tLoss 0.009 | Accuracy: 1.000\n","Test:\tLoss 0.449 | Accuracy: 0.950\n","--- Epoch 900\n","Train:\tLoss 0.005 | Accuracy: 1.000\n","Test:\tLoss 0.451 | Accuracy: 0.900\n","--- Epoch 1000\n","Train:\tLoss 0.003 | Accuracy: 1.000\n","Test:\tLoss 0.448 | Accuracy: 0.900\n"],"name":"stdout"}]}]}